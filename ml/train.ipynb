{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1185c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp313-cp313-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp313-cp313-macosx_10_13_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]7\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a72694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.0-cp313-cp313-macosx_12_0_arm64.whl (10.6 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.0-cp313-cp313-macosx_14_0_arm64.whl (20.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406168b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.3.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached numpy-2.3.1-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.1 pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc66e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2007 entries, 0 to 2006\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   PetID               2007 non-null   int64  \n",
      " 1   PetType             2007 non-null   object \n",
      " 2   Breed               2007 non-null   object \n",
      " 3   AgeMonths           2007 non-null   int64  \n",
      " 4   Color               2007 non-null   object \n",
      " 5   Size                2007 non-null   object \n",
      " 6   WeightKg            2007 non-null   float64\n",
      " 7   Vaccinated          2007 non-null   int64  \n",
      " 8   HealthCondition     2007 non-null   int64  \n",
      " 9   TimeInShelterDays   2007 non-null   int64  \n",
      " 10  AdoptionFee         2007 non-null   int64  \n",
      " 11  PreviousOwner       2007 non-null   int64  \n",
      " 12  AdoptionLikelihood  2007 non-null   int64  \n",
      "dtypes: float64(1), int64(8), object(4)\n",
      "memory usage: 204.0+ KB\n",
      "None\n",
      "   PetID PetType             Breed  AgeMonths   Color    Size   WeightKg  \\\n",
      "0    500    Bird          Parakeet        131  Orange   Large   5.039768   \n",
      "1    501  Rabbit            Rabbit         73   White   Large  16.086727   \n",
      "2    502     Dog  Golden Retriever        136  Orange  Medium   2.076286   \n",
      "3    503    Bird          Parakeet         97   White   Small   3.339423   \n",
      "4    504  Rabbit            Rabbit        123    Gray   Large  20.498100   \n",
      "\n",
      "   Vaccinated  HealthCondition  TimeInShelterDays  AdoptionFee  PreviousOwner  \\\n",
      "0           1                0                 27          140              0   \n",
      "1           0                0                  8          235              0   \n",
      "2           0                0                 85          385              0   \n",
      "3           0                0                 61          217              1   \n",
      "4           0                0                 28           14              1   \n",
      "\n",
      "   AdoptionLikelihood  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n",
      "AdoptionLikelihood\n",
      "0    1348\n",
      "1     659\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/pet_adoption_data.csv')\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df['AdoptionLikelihood'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5520fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetID                 0\n",
      "PetType               0\n",
      "Breed                 0\n",
      "AgeMonths             0\n",
      "Color                 0\n",
      "Size                  0\n",
      "WeightKg              0\n",
      "Vaccinated            0\n",
      "HealthCondition       0\n",
      "TimeInShelterDays     0\n",
      "AdoptionFee           0\n",
      "PreviousOwner         0\n",
      "AdoptionLikelihood    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntCastingNaNError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m binary_map = {\u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNo\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m}\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mVaccinated\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDewormed\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSterilized\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     df[col] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Map other categorical columns\u001b[39;00m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# PetType: Dog=0, Cat=1 (assuming just these two)\u001b[39;00m\n\u001b[32m     32\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mPetType\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mPetType\u001b[39m\u001b[33m'\u001b[39m].map({\u001b[33m'\u001b[39m\u001b[33mDog\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCat\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/generic.py:6662\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6656\u001b[39m     results = [\n\u001b[32m   6657\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6658\u001b[39m     ]\n\u001b[32m   6660\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6661\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6662\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6663\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:101\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.ensure_string_array(\n\u001b[32m     97\u001b[39m         arr, skipna=skipna, convert_na_value=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     98\u001b[39m     ).reshape(shape)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m np.issubdtype(arr.dtype, np.floating) \u001b[38;5;129;01mand\u001b[39;00m dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_astype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_np_dtype(dtype, \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:145\u001b[39m, in \u001b[36m_astype_float_to_int_nansafe\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(values).all():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values >= \u001b[32m0\u001b[39m).all():\n",
      "\u001b[31mIntCastingNaNError\u001b[39m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/pet_adoption_data.csv')\n",
    "\n",
    "# Quick look at missing data\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing numeric or categorical columns (choose a strategy)\n",
    "# For simplicity, fill missing values with 0 or 'None' for categorical\n",
    "df.fillna({\n",
    "    'Breed2': 0,              # If breed2 missing, assign 0 (means no second breed)\n",
    "    'Color2': 0,\n",
    "    'Color3': 0,\n",
    "    'FurLength': 0,\n",
    "    'Vaccinated': 0,\n",
    "    'Dewormed': 0,\n",
    "    'Sterilized': 0,\n",
    "    'Health': 0,\n",
    "}, inplace=True)\n",
    "\n",
    "# Map binary categorical columns manually\n",
    "binary_map = {'Yes': 1, 'No': 0, 'Unknown': 0}\n",
    "\n",
    "for col in ['Vaccinated', 'Dewormed', 'Sterilized']:\n",
    "    df[col] = df[col].map(binary_map).astype(int)\n",
    "\n",
    "# Map other categorical columns\n",
    "\n",
    "# PetType: Dog=0, Cat=1 (assuming just these two)\n",
    "df['PetType'] = df['PetType'].map({'Dog': 0, 'Cat': 1})\n",
    "\n",
    "# Gender: Male=0, Female=1, Unknown=0 (treat unknown as 0)\n",
    "df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1, 'Unknown': 0})\n",
    "\n",
    "# MaturitySize, FurLength, Health are numeric but might be strings, ensure int\n",
    "df['MaturitySize'] = pd.to_numeric(df['MaturitySize'], errors='coerce').fillna(0).astype(int)\n",
    "df['FurLength'] = pd.to_numeric(df['FurLength'], errors='coerce').fillna(0).astype(int)\n",
    "df['Health'] = pd.to_numeric(df['Health'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Breeds and Colors are already numeric but if strings, encode or convert\n",
    "df['Breed1'] = pd.to_numeric(df['Breed1'], errors='coerce').fillna(0).astype(int)\n",
    "df['Breed2'] = pd.to_numeric(df['Breed2'], errors='coerce').fillna(0).astype(int)\n",
    "df['Color1'] = pd.to_numeric(df['Color1'], errors='coerce').fillna(0).astype(int)\n",
    "df['Color2'] = pd.to_numeric(df['Color2'], errors='coerce').fillna(0).astype(int)\n",
    "df['Color3'] = pd.to_numeric(df['Color3'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Age: convert to int (months)\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Select features to keep\n",
    "features = ['PetType', 'Age', 'Breed1', 'Breed2', 'Gender',\n",
    "            'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength',\n",
    "            'Vaccinated', 'Dewormed']\n",
    "\n",
    "# Target variable\n",
    "target = 'AdoptionLikelihood'\n",
    "\n",
    "# Final feature and target sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "print(X.head())\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04630a",
   "metadata": {},
   "source": [
    "UNDERSAMPLE AND BALANCE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69696862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Balanced dataset shape: (1785, 20)\n",
      "\n",
      "📊 Balanced AdoptionSpeed distribution:\n",
      " AdoptionSpeed\n",
      "3    357\n",
      "4    357\n",
      "0    357\n",
      "2    357\n",
      "1    357\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate by class\n",
    "df_0 = df[df[\"AdoptionSpeed\"] == 0]\n",
    "df_1 = df[df[\"AdoptionSpeed\"] == 1]\n",
    "df_2 = df[df[\"AdoptionSpeed\"] == 2]\n",
    "df_3 = df[df[\"AdoptionSpeed\"] == 3]\n",
    "df_4 = df[df[\"AdoptionSpeed\"] == 4]\n",
    "\n",
    "# Downsample to match class 0 (357 samples)\n",
    "df_1_bal = resample(df_1, replace=False, n_samples=357, random_state=42)\n",
    "df_2_bal = resample(df_2, replace=False, n_samples=357, random_state=42)\n",
    "df_3_bal = resample(df_3, replace=False, n_samples=357, random_state=42)\n",
    "df_4_bal = resample(df_4, replace=False, n_samples=357, random_state=42)\n",
    "\n",
    "# Combine all balanced subsets\n",
    "df_balanced = pd.concat([df_0, df_1_bal, df_2_bal, df_3_bal, df_4_bal])\n",
    "\n",
    "# Shuffle dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"✅ Balanced dataset shape:\", df_balanced.shape)\n",
    "print(\"\\n📊 Balanced AdoptionSpeed distribution:\\n\", df_balanced[\"AdoptionSpeed\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc76e64",
   "metadata": {},
   "source": [
    "RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01d9fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌳 Random Forest Accuracy on balanced data: 24.65%\n",
      "\n",
      "📄 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.38      0.35        69\n",
      "           1       0.26      0.24      0.25        79\n",
      "           2       0.15      0.16      0.16        67\n",
      "           3       0.23      0.19      0.21        77\n",
      "           4       0.25      0.26      0.26        65\n",
      "\n",
      "    accuracy                           0.25       357\n",
      "   macro avg       0.25      0.25      0.25       357\n",
      "weighted avg       0.25      0.25      0.25       357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 🔄 Reuse your balanced dataframe\n",
    "df = df_balanced.copy()\n",
    "\n",
    "# Drop unused columns if still present (safety check)\n",
    "df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\"], errors=\"ignore\")\n",
    "\n",
    "# Map categorical fields\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"🌳 Random Forest Accuracy on balanced data: {acc*100:.2f}%\")\n",
    "print(\"\\n📄 Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0e34fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Preprocessing complete!\n",
      "Training samples: 11994, Testing samples: 2999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Drop unused or complex fields\n",
    "df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\"])\n",
    "\n",
    "# Map categorical fields manually (adjust mappings as per your dataset)\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})  # Male, Female, Mixed\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})  # Healthy, Minor injury, Serious\n",
    "\n",
    "# Features and label\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "# Scale features to [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train/test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Preprocessing complete!\")\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ed97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Preprocessing complete!\n",
      "Training samples: 11994, Testing samples: 2999\n",
      "k=1: Accuracy = 21.37%\n",
      "k=2: Accuracy = 21.37%\n",
      "k=3: Accuracy = 21.37%\n",
      "k=4: Accuracy = 21.37%\n",
      "k=5: Accuracy = 21.37%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 53\u001b[39m\n",
      "\u001b[32m     51\u001b[39m total = \u001b[38;5;28mlen\u001b[39m(X_test)\n",
      "\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total):\n",
      "\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     pred = \u001b[43mknn_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pred == y_test.iloc[i]:\n",
      "\u001b[32m     55\u001b[39m         correct += \u001b[32m1\u001b[39m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mknn_predict\u001b[39m\u001b[34m(X_train, y_train, X_test_instance, k)\u001b[39m\n",
      "\u001b[32m     36\u001b[39m distances = []\n",
      "\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n",
      "\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     dist = \u001b[43meuclidean_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_instance\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     39\u001b[39m     distances.append((dist, y_train.iloc[i]))\n",
      "\u001b[32m     40\u001b[39m distances.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36meuclidean_distance\u001b[39m\u001b[34m(a, b)\u001b[39m\n",
      "\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meuclidean_distance\u001b[39m(a, b):\n",
      "\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n",
      "\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n",
      "\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:70\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n",
      "\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, **kwargs):\n",
      "\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     passkwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n",
      "\u001b[32m     71\u001b[39m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue}\n",
      "\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu.ndarray:\n",
      "\u001b[32m     74\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# --- Your existing preprocessing ---\n",
    "# Drop unused or complex fields\n",
    "#df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\"])\n",
    "\n",
    "# Map categorical fields manually\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})\n",
    "\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Preprocessing complete!\")\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# --- Add this block below to tune k and test accuracy ---\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train.iloc[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "accuracies = []\n",
    "k_values = range(1, 21)\n",
    "\n",
    "for k in k_values:\n",
    "    correct = 0\n",
    "    total = len(X_test)\n",
    "    for i in range(total):\n",
    "        pred = knn_predict(X_train, y_train, X_test[i], k)\n",
    "        if pred == y_test.iloc[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"k={k}: Accuracy = {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8f6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 KNN Accuracy on first 100 test samples: 30.00%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Euclidean distance function\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "# KNN prediction function\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train.iloc[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "# Test KNN on first 100 samples\n",
    "correct = 0\n",
    "total = 400\n",
    "\n",
    "for i in range(total):\n",
    "    prediction = knn_predict(X_train, y_train, X_test[i], k=5)\n",
    "    actual = y_test.iloc[i]\n",
    "    if prediction == actual:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\n🔎 KNN Accuracy on first {total} test samples: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852229ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN on full dataset with 14993 samples...\n",
      "✅ Training data and scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Drop unused or complex fields\n",
    "df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\"])\n",
    "\n",
    "# Map categorical fields manually (adjust as per your dataset)\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})  # Male, Female, Mixed\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})  # Healthy, Minor injury, Serious\n",
    "\n",
    "# Features and label\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "# Scale features to [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_full = y.values\n",
    "\n",
    "print(f\"Training KNN on full dataset with {X_scaled.shape[0]} samples...\")\n",
    "\n",
    "# Save the scaled features, labels, and scaler for later use\n",
    "with open('knn_X_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(X_scaled, f)\n",
    "\n",
    "with open('knn_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y_full, f)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✅ Training data and scaler saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c270941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'VideoAmt', 'PhotoAmt']\n"
     ]
    }
   ],
   "source": [
    "# After preprocessing and dropping columns, print features like this:\n",
    "print(X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a8a7d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 19 features, but MinMaxScaler is expecting 17 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     33\u001b[39m new_pet = np.array([[\n\u001b[32m     34\u001b[39m     \u001b[32m1\u001b[39m,     \u001b[38;5;66;03m# Type\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[32m1\u001b[39m,     \u001b[38;5;66;03m# Age\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     \u001b[32m1\u001b[39m      \u001b[38;5;66;03m# PhotoAmt\u001b[39;00m\n\u001b[32m     53\u001b[39m ]])\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Scale and predict\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m new_pet_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m prediction = knn_predict(X_train, y_train, new_pet_scaled[\u001b[32m0\u001b[39m], k=\u001b[32m5\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔮 Predicted Adoption Speed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:545\u001b[39m, in \u001b[36mMinMaxScaler.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    541\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    543\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_array_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m X *= \u001b[38;5;28mself\u001b[39m.scale_\n\u001b[32m    556\u001b[39m X += \u001b[38;5;28mself\u001b[39m.min_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 19 features, but MinMaxScaler is expecting 17 features as input."
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 🔮 Predicting a New Pet's Adoption Speed\n",
    "# -------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "# Load training data and scaler\n",
    "with open('knn_X_scaled.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('knn_y.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Provide a new pet's feature vector (must match the 19-feature structure)\n",
    "new_pet = np.array([[\n",
    "    1,     # Type\n",
    "    1,     # Age\n",
    "    307,   # Breed1\n",
    "    0,     # Breed2\n",
    "    2,     # Gender\n",
    "    1,     # Color1\n",
    "    0,     # Color2\n",
    "    0,     # Color3\n",
    "    1,     # MaturitySize\n",
    "    2,     # FurLength\n",
    "    1,     # Vaccinated\n",
    "    1,     # Dewormed\n",
    "    1,     # Sterilized\n",
    "    0,     # Health\n",
    "    1,     # Quantity\n",
    "    100,   # Fee\n",
    "    41326, # State\n",
    "    0,     # VideoAmt\n",
    "    1      # PhotoAmt\n",
    "]])\n",
    "\n",
    "# Scale and predict\n",
    "new_pet_scaled = scaler.transform(new_pet)\n",
    "prediction = knn_predict(X_train, y_train, new_pet_scaled[0], k=5)\n",
    "print(f\"🔮 Predicted Adoption Speed: {prediction}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e633266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 19 features, but MinMaxScaler is expecting 17 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     33\u001b[39m new_pet = np.array([[\n\u001b[32m     34\u001b[39m     \u001b[32m1\u001b[39m,      \u001b[38;5;66;03m# Dog\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[32m1\u001b[39m,      \u001b[38;5;66;03m# Age: 1 month\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     \u001b[32m3\u001b[39m       \u001b[38;5;66;03m# 3 photos\u001b[39;00m\n\u001b[32m     53\u001b[39m ]])\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Scale and predict\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m new_pet_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m prediction = knn_predict(X_train, y_train, new_pet_scaled[\u001b[32m0\u001b[39m], k=\u001b[32m5\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔮 Predicted Adoption Speed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:545\u001b[39m, in \u001b[36mMinMaxScaler.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    541\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    543\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_array_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m X *= \u001b[38;5;28mself\u001b[39m.scale_\n\u001b[32m    556\u001b[39m X += \u001b[38;5;28mself\u001b[39m.min_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 19 features, but MinMaxScaler is expecting 17 features as input."
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 🔮 Predicting a New Pet's Adoption Speed\n",
    "# -------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "# Load training data and scaler\n",
    "with open('knn_X_scaled.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('knn_y.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Provide a new pet's feature vector (must match the 19-feature structure)\n",
    "new_pet = np.array([[\n",
    "    1,      # Dog\n",
    "    1,      # Age: 1 month\n",
    "    307,    # Breed1: Popular breed\n",
    "    0,      # Breed2: None\n",
    "    2,      # Gender: Female\n",
    "    1,      # Color1: Brown\n",
    "    0,      # Color2: None\n",
    "    0,      # Color3: None\n",
    "    1,      # Small size\n",
    "    1,      # Short fur\n",
    "    1,      # Vaccinated\n",
    "    1,      # Dewormed\n",
    "    1,      # Sterilized\n",
    "    0,      # Healthy\n",
    "    1,      # Quantity: 1\n",
    "    0,      # Free adoption\n",
    "    41326,  # State\n",
    "    0,      # No video\n",
    "    3       # 3 photos\n",
    "]])\n",
    "\n",
    "# Scale and predict\n",
    "new_pet_scaled = scaler.transform(new_pet)\n",
    "prediction = knn_predict(X_train, y_train, new_pet_scaled[0], k=5)\n",
    "print(f\"🔮 Predicted Adoption Speed: {prediction}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dbdd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "944d2349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN on full dataset with 14993 samples...\n",
      "✅ Training data and scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Drop unused or complex fields + 'State' and 'Fee'\n",
    "df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\", \"State\", \"Fee\"])\n",
    "\n",
    "# Map categorical fields manually\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})\n",
    "\n",
    "# Features and label\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "# Scale features to [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_full = y.values\n",
    "\n",
    "print(f\"Training KNN on full dataset with {X_scaled.shape[0]} samples...\")\n",
    "\n",
    "# Save the scaled features, labels, and scaler for later use\n",
    "with open('knn_X_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(X_scaled, f)\n",
    "with open('knn_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y_full, f)\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✅ Training data and scaler saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7be7c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Predicted Adoption Speed: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "# Load training data and scaler\n",
    "with open('knn_X_scaled.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('knn_y.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Column names after dropping 'State' and 'Fee'\n",
    "columns = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3',\n",
    "           'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n",
    "           'Health', 'Quantity', 'VideoAmt', 'PhotoAmt']\n",
    "\n",
    "# New pet example WITHOUT 'State' and 'Fee'\n",
    "new_pet = np.array([[\n",
    "    1,      # Dog\n",
    "    1,      # Age: 1 month\n",
    "    307,    # Breed1: Popular breed\n",
    "    0,      # Breed2: None\n",
    "    2,      # Gender: Female\n",
    "    1,      # Color1: Brown\n",
    "    0,      # Color2: None\n",
    "    0,      # Color3: None\n",
    "    1,      # Small size\n",
    "    1,      # Short fur\n",
    "    1,      # Vaccinated\n",
    "    1,      # Dewormed\n",
    "    1,      # Sterilized\n",
    "    0,      # Healthy\n",
    "    1,      # Quantity: 1\n",
    "    0,      # No video\n",
    "    3       # 3 photos\n",
    "]])\n",
    "\n",
    "# Convert to DataFrame to keep feature names (avoid sklearn warning)\n",
    "new_pet_df = pd.DataFrame(new_pet, columns=columns)\n",
    "\n",
    "# Scale the new pet features\n",
    "new_pet_scaled = scaler.transform(new_pet_df)\n",
    "\n",
    "# Predict adoption speed\n",
    "prediction = knn_predict(X_train, y_train, new_pet_scaled[0], k=9)\n",
    "print(f\"🔮 Predicted Adoption Speed: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1814b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Manual KNN predicted adoption speed: 4\n",
      "🔮 Sklearn KNN predicted adoption speed: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "# Load training data and scaler\n",
    "with open('knn_X_scaled.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('knn_y.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Define feature columns used in training (adjust if you changed features)\n",
    "columns = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3',\n",
    "           'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n",
    "           'Health', 'Quantity', 'VideoAmt', 'PhotoAmt']\n",
    "\n",
    "# New pet features as list of lists (19 features)\n",
    "new_pet = [[\n",
    "    1,      # Dog\n",
    "    1,      # Age\n",
    "    307,    # Breed1\n",
    "    0,      # Breed2\n",
    "    2,      # Gender\n",
    "    1,      # Color1\n",
    "    0,      # Color2\n",
    "    0,      # Color3\n",
    "    1,      # MaturitySize\n",
    "    1,      # FurLength\n",
    "    1,      # Vaccinated\n",
    "    1,      # Dewormed\n",
    "    1,      # Sterilized\n",
    "    0,      # Health\n",
    "    1,      # Quantity\n",
    "    0,      # VideoAmt\n",
    "    3       # PhotoAmt\n",
    "]]\n",
    "\n",
    "# Convert to DataFrame for proper column names\n",
    "new_pet_df = pd.DataFrame(new_pet, columns=columns)\n",
    "\n",
    "# Scale new pet features\n",
    "new_pet_scaled = scaler.transform(new_pet_df)\n",
    "\n",
    "# Manual KNN prediction\n",
    "manual_prediction = knn_predict(X_train, y_train, new_pet_scaled[0], k=5)\n",
    "print(f\"🔮 Manual KNN predicted adoption speed: {manual_prediction}\")\n",
    "\n",
    "# Sklearn KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "sklearn_prediction = knn.predict(new_pet_scaled)\n",
    "print(f\"🔮 Sklearn KNN predicted adoption speed: {sklearn_prediction[0]}\")\n",
    "\n",
    "# Optional: Tune k and see which k gives better accuracy on a test set\n",
    "# You can do this by splitting X_train/y_train further into train/test and trying multiple k values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1beaee",
   "metadata": {},
   "source": [
    "KUN K KO VALUE RAMRO XA HERNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40c0e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Preprocessing complete!\n",
      "Training samples: 11994, Testing samples: 2999\n",
      "k=1: Accuracy = 21.37%\n",
      "k=2: Accuracy = 21.37%\n",
      "k=3: Accuracy = 21.37%\n",
      "k=4: Accuracy = 21.37%\n",
      "k=5: Accuracy = 21.37%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m total = \u001b[38;5;28mlen\u001b[39m(X_test)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     pred = \u001b[43mknn_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pred == y_test.iloc[i]:\n\u001b[32m     55\u001b[39m         correct += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mknn_predict\u001b[39m\u001b[34m(X_train, y_train, X_test_instance, k)\u001b[39m\n\u001b[32m     36\u001b[39m distances = []\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     dist = \u001b[43meuclidean_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     distances.append((dist, y_train.iloc[i]))\n\u001b[32m     40\u001b[39m distances.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36meuclidean_distance\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meuclidean_distance\u001b[39m(a, b):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:70\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     passkwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m     71\u001b[39m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue}\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu.ndarray:\n\u001b[32m     74\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# --- Your existing preprocessing ---\n",
    "# Drop unused or complex fields\n",
    "#df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\"])\n",
    "\n",
    "# Map categorical fields manually\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})\n",
    "\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Preprocessing complete!\")\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# --- Add this block below to tune k and test accuracy ---\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train.iloc[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "accuracies = []\n",
    "k_values = range(1, 21)\n",
    "\n",
    "for k in k_values:\n",
    "    correct = 0\n",
    "    total = len(X_test)\n",
    "    for i in range(total):\n",
    "        pred = knn_predict(X_train, y_train, X_test[i], k)\n",
    "        if pred == y_test.iloc[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"k={k}: Accuracy = {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db50096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11994, Testing samples: 2999\n",
      "k=1: Accuracy = 31.44%\n",
      "k=2: Accuracy = 28.98%\n",
      "k=3: Accuracy = 30.24%\n",
      "k=4: Accuracy = 31.21%\n",
      "k=5: Accuracy = 31.88%\n",
      "k=6: Accuracy = 31.98%\n",
      "k=7: Accuracy = 31.34%\n",
      "k=8: Accuracy = 32.01%\n",
      "k=9: Accuracy = 32.28%\n",
      "k=10: Accuracy = 32.64%\n",
      "k=11: Accuracy = 32.58%\n",
      "k=12: Accuracy = 32.84%\n",
      "k=13: Accuracy = 32.88%\n",
      "k=14: Accuracy = 32.21%\n",
      "k=15: Accuracy = 32.54%\n",
      "k=16: Accuracy = 32.91%\n",
      "k=17: Accuracy = 32.91%\n",
      "k=18: Accuracy = 34.21%\n",
      "k=19: Accuracy = 33.31%\n",
      "k=20: Accuracy = 33.38%\n",
      "k=21: Accuracy = 32.78%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Drop unused or complex fields\n",
    "df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\"])\n",
    "\n",
    "# Map categorical fields manually\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})\n",
    "\n",
    "# Features and label\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "# Impute missing values with most frequent (mode)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Scale features to [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Split into train/test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y.values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Tune k from 1 to 21 and check accuracy\n",
    "for k in range(1, 22):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"k={k}: Accuracy = {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6adbca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11994, Testing samples: 2999\n",
      "Random Forest Accuracy: 38.35%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.04      0.08        92\n",
      "           1       0.35      0.33      0.34       627\n",
      "           2       0.34      0.36      0.35       806\n",
      "           3       0.31      0.25      0.28       641\n",
      "           4       0.47      0.59      0.53       833\n",
      "\n",
      "    accuracy                           0.38      2999\n",
      "   macro avg       0.35      0.31      0.31      2999\n",
      "weighted avg       0.37      0.38      0.37      2999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Drop unused or complex columns\n",
    "df = df.drop(columns=[\"Name\", \"PetID\", \"RescuerID\", \"Description\", \"State\", \"Fee\"])\n",
    "\n",
    "# Map categorical fields manually (adjust as per dataset)\n",
    "df[\"Gender\"] = df[\"Gender\"].map({1: 0, 2: 1, 3: 2})\n",
    "df[\"Vaccinated\"] = df[\"Vaccinated\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Dewormed\"] = df[\"Dewormed\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Sterilized\"] = df[\"Sterilized\"].map({1: 1, 2: 0, 3: 0})\n",
    "df[\"Health\"] = df[\"Health\"].map({1: 0, 2: 1, 3: 2})\n",
    "\n",
    "# Features and label\n",
    "X = df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y = df[\"AdoptionSpeed\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Initialize Random Forest with some params (you can tune these later)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Detailed classification report (precision, recall, f1-score per class)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model and scaler for later use\n",
    "with open(\"rf_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf, f)\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130586bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "286419e9",
   "metadata": {},
   "source": [
    "arko dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdfcbb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2007 entries, 0 to 2006\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   PetID               2007 non-null   int64  \n",
      " 1   PetType             2007 non-null   object \n",
      " 2   Breed               2007 non-null   object \n",
      " 3   AgeMonths           2007 non-null   int64  \n",
      " 4   Color               2007 non-null   object \n",
      " 5   Size                2007 non-null   object \n",
      " 6   WeightKg            2007 non-null   float64\n",
      " 7   Vaccinated          2007 non-null   int64  \n",
      " 8   HealthCondition     2007 non-null   int64  \n",
      " 9   TimeInShelterDays   2007 non-null   int64  \n",
      " 10  AdoptionFee         2007 non-null   int64  \n",
      " 11  PreviousOwner       2007 non-null   int64  \n",
      " 12  AdoptionLikelihood  2007 non-null   int64  \n",
      "dtypes: float64(1), int64(8), object(4)\n",
      "memory usage: 204.0+ KB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "   PetID PetType             Breed  AgeMonths   Color    Size   WeightKg  \\\n",
      "0    500    Bird          Parakeet        131  Orange   Large   5.039768   \n",
      "1    501  Rabbit            Rabbit         73   White   Large  16.086727   \n",
      "2    502     Dog  Golden Retriever        136  Orange  Medium   2.076286   \n",
      "3    503    Bird          Parakeet         97   White   Small   3.339423   \n",
      "4    504  Rabbit            Rabbit        123    Gray   Large  20.498100   \n",
      "\n",
      "   Vaccinated  HealthCondition  TimeInShelterDays  AdoptionFee  PreviousOwner  \\\n",
      "0           1                0                 27          140              0   \n",
      "1           0                0                  8          235              0   \n",
      "2           0                0                 85          385              0   \n",
      "3           0                0                 61          217              1   \n",
      "4           0                0                 28           14              1   \n",
      "\n",
      "   AdoptionLikelihood  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n",
      "\n",
      "Summary statistics:\n",
      "              PetID PetType   Breed    AgeMonths  Color    Size     WeightKg  \\\n",
      "count   2007.000000    2007    2007  2007.000000   2007    2007  2007.000000   \n",
      "unique          NaN       4       7          NaN      5       3          NaN   \n",
      "top             NaN     Dog  Rabbit          NaN  White  Medium          NaN   \n",
      "freq            NaN     522     493          NaN    420     714          NaN   \n",
      "mean    1503.000000     NaN     NaN    92.279522    NaN     NaN    15.705776   \n",
      "std      579.515315     NaN     NaN    52.148363    NaN     NaN     8.327749   \n",
      "min      500.000000     NaN     NaN     1.000000    NaN     NaN     1.018198   \n",
      "25%     1001.500000     NaN     NaN    48.000000    NaN     NaN     8.730396   \n",
      "50%     1503.000000     NaN     NaN    94.000000    NaN     NaN    15.925416   \n",
      "75%     2004.500000     NaN     NaN   138.000000    NaN     NaN    22.737180   \n",
      "max     2506.000000     NaN     NaN   179.000000    NaN     NaN    29.995628   \n",
      "\n",
      "         Vaccinated  HealthCondition  TimeInShelterDays  AdoptionFee  \\\n",
      "count   2007.000000      2007.000000        2007.000000  2007.000000   \n",
      "unique          NaN              NaN                NaN          NaN   \n",
      "top             NaN              NaN                NaN          NaN   \n",
      "freq            NaN              NaN                NaN          NaN   \n",
      "mean       0.701046         0.196313          43.974091   249.142003   \n",
      "std        0.457914         0.397307          25.740253   142.887040   \n",
      "min        0.000000         0.000000           1.000000     0.000000   \n",
      "25%        0.000000         0.000000          21.000000   127.000000   \n",
      "50%        1.000000         0.000000          45.000000   242.000000   \n",
      "75%        1.000000         0.000000          66.000000   375.000000   \n",
      "max        1.000000         1.000000          89.000000   499.000000   \n",
      "\n",
      "        PreviousOwner  AdoptionLikelihood  \n",
      "count     2007.000000         2007.000000  \n",
      "unique            NaN                 NaN  \n",
      "top               NaN                 NaN  \n",
      "freq              NaN                 NaN  \n",
      "mean         0.301943            0.328351  \n",
      "std          0.459215            0.469730  \n",
      "min          0.000000            0.000000  \n",
      "25%          0.000000            0.000000  \n",
      "50%          0.000000            0.000000  \n",
      "75%          1.000000            1.000000  \n",
      "max          1.000000            1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset\n",
    "df = pd.read_csv(\"data/pet_adoption_data.csv\")\n",
    "\n",
    "# Show basic info and first few rows\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c61532aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "PetID                 0\n",
      "PetType               0\n",
      "Breed                 0\n",
      "AgeMonths             0\n",
      "Color                 0\n",
      "Size                  0\n",
      "WeightKg              0\n",
      "Vaccinated            0\n",
      "HealthCondition       0\n",
      "TimeInShelterDays     0\n",
      "AdoptionFee           0\n",
      "PreviousOwner         0\n",
      "AdoptionLikelihood    0\n",
      "dtype: int64\n",
      "\n",
      "Sample preprocessed data:\n",
      "   PetType  Breed  AgeMonths  Color  Size  WeightKg  Vaccinated  \\\n",
      "0        0      2   0.730337      3     0  0.138783           1   \n",
      "1        3      5   0.404494      4     0  0.520009           0   \n",
      "2        2      0   0.758427      3     1  0.036514           0   \n",
      "3        0      2   0.539326      4     2  0.080105           0   \n",
      "4        3      5   0.685393      2     0  0.672244           0   \n",
      "\n",
      "   HealthCondition  TimeInShelterDays  AdoptionFee  PreviousOwner  \n",
      "0                0           0.295455     0.280561              0  \n",
      "1                0           0.079545     0.470942              0  \n",
      "2                0           0.954545     0.771543              0  \n",
      "3                0           0.681818     0.434870              1  \n",
      "4                0           0.306818     0.028056              1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/pet_adoption_data.csv\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Encode categorical variables using LabelEncoder for simplicity\n",
    "label_encoders = {}\n",
    "for col in ['PetType', 'Breed', 'Color', 'Size']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Features and label\n",
    "X = df.drop(columns=['PetID', 'AdoptionLikelihood'])\n",
    "y = df['AdoptionLikelihood']\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X[['AgeMonths', 'WeightKg', 'TimeInShelterDays', 'AdoptionFee']] = scaler.fit_transform(\n",
    "    X[['AgeMonths', 'WeightKg', 'TimeInShelterDays', 'AdoptionFee']]\n",
    ")\n",
    "\n",
    "print(\"\\nSample preprocessed data:\")\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ca921a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 90.30%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       270\n",
      "           1       0.88      0.81      0.85       132\n",
      "\n",
      "    accuracy                           0.90       402\n",
      "   macro avg       0.90      0.88      0.89       402\n",
      "weighted avg       0.90      0.90      0.90       402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {acc*100:.2f}%\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d2c7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (k=5): 81.09%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your data is in pandas df with features X and target y\n",
    "\n",
    "# Preprocess your data (convert categorical to numeric as needed)\n",
    "# For example, encode categorical columns manually or via pandas get_dummies\n",
    "\n",
    "# Example minimal preprocessing:\n",
    "# df['PetType'] = df['PetType'].map({'Dog': 0, 'Cat': 1})\n",
    "# (Do similar for other categorical features or use label encoding)\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['AdoptionLikelihood'])  # change target col as per your dataset\n",
    "y = df['AdoptionLikelihood']\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train.iloc[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "# Predict for the test set\n",
    "y_pred = []\n",
    "for x in X_test:\n",
    "    pred = knn_predict(X_train, y_train, x, k=5)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(np.array(y_pred) == y_test.values) / len(y_test)\n",
    "print(f\"KNN Accuracy (k=5): {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7e5e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors: [(np.float64(36.07227229893301), np.int64(0)), (np.float64(36.07534991409731), np.int64(0)), (np.float64(36.076553727378815), np.int64(0)), (np.float64(36.07788097911264), np.int64(0)), (np.float64(36.08027833060056), np.int64(0))]\n",
      "🔮 Predicted Adoption Speed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nisla/Desktop/furever-home/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suppose these are your training features in order:\n",
    "feature_order = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed']\n",
    "\n",
    "# Create one sample with values matching each feature (encoded / numeric values)\n",
    "new_pet_sample = np.array([[ \n",
    "    4,   # Type\n",
    "    111,  # Age (months)\n",
    "    3,   # Breed1\n",
    "    0,   # Breed2\n",
    "    1,   # Gender\n",
    "    5,   # Color1\n",
    "    1,   # Color2\n",
    "    0,   # Color3\n",
    "    2,   # MaturitySize\n",
    "    1,   # FurLength\n",
    "    1,   # Vaccinated\n",
    "    0    # Dewormed\n",
    "]])\n",
    "\n",
    "# Scale new input\n",
    "new_pet_scaled = scaler.transform(new_pet_sample)\n",
    "\n",
    "# Predict using your manual KNN function or model\n",
    "prediction = knn_predict(X_train, y_train, new_pet_scaled[0], k=5)\n",
    "\n",
    "\n",
    "print(f\"🔮 Predicted Adoption Speed: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15df63e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Suppose these are your 12 feature columns (replace with your actual columns)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Then create new sample with exactly those columns\u001b[39;00m\n\u001b[32m      5\u001b[39m new_pet_sample_2 = pd.DataFrame([[\n\u001b[32m      6\u001b[39m     \u001b[32m1\u001b[39m,      \u001b[38;5;66;03m# PetType\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[32m5\u001b[39m,      \u001b[38;5;66;03m# Breed\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[32m0\u001b[39m       \u001b[38;5;66;03m# (last feature, e.g. if exists)\u001b[39;00m\n\u001b[32m     18\u001b[39m ]], columns=X_train.columns)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Suppose these are your 12 feature columns (replace with your actual columns)\n",
    "print(X_train.columns)\n",
    "\n",
    "# Then create new sample with exactly those columns\n",
    "new_pet_sample_2 = pd.DataFrame([[\n",
    "    1,      # PetType\n",
    "    5,      # Breed\n",
    "    3,      # AgeMonths\n",
    "    2,      # Color\n",
    "    1,      # Size\n",
    "    5.5,    # WeightKg\n",
    "    1,      # Vaccinated\n",
    "    0,      # HealthCondition\n",
    "    10,     # TimeInShelterDays\n",
    "    100,    # AdoptionFee\n",
    "    0,      # PreviousOwner\n",
    "    0       # (last feature, e.g. if exists)\n",
    "]], columns=X_train.columns)\n",
    "\n",
    "# Now scale and predict\n",
    "new_pet_scaled_2 = scaler.transform(new_pet_sample_2)\n",
    "prediction_2 = knn_predict(X_train.values, y_train, new_pet_scaled_2[0], k=5)\n",
    "\n",
    "print(f\"🔮 Predicted Adoption Speed for new sample: {prediction_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9842147a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m.tolist())  \u001b[38;5;66;03m# List of feature names used in training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(X_train.shape)             \u001b[38;5;66;03m# Confirm feature count\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(scaler.n_features_in_)     \u001b[38;5;66;03m# Number of features scaler expects\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "print(X_train.columns.tolist())  # List of feature names used in training\n",
    "print(X_train.shape)             # Confirm feature count\n",
    "print(scaler.n_features_in_)     # Number of features scaler expects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d59b4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train, y_train, X_test_instance, k=5):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], X_test_instance)\n",
    "        distances.append((dist, y_train.iloc[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    print(\"Neighbors:\", k_nearest)  # DEBUG neighbors and their labels\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "    return most_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d277969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdoptionLikelihood\n",
      "0    1078\n",
      "1     527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cef16",
   "metadata": {},
   "source": [
    "arko data set again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4ae5748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 722 entries, 0 to 721\n",
      "Data columns (total 26 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      722 non-null    int64  \n",
      " 1   organization_id         722 non-null    object \n",
      " 2   url                     722 non-null    object \n",
      " 3   type                    722 non-null    object \n",
      " 4   species                 722 non-null    object \n",
      " 5   breeds                  722 non-null    object \n",
      " 6   colors                  722 non-null    object \n",
      " 7   age                     722 non-null    object \n",
      " 8   gender                  722 non-null    object \n",
      " 9   size                    722 non-null    object \n",
      " 10  coat                    397 non-null    object \n",
      " 11  attributes              722 non-null    object \n",
      " 12  environment             722 non-null    object \n",
      " 13  tags                    722 non-null    object \n",
      " 14  name                    722 non-null    object \n",
      " 15  description             658 non-null    object \n",
      " 16  organization_animal_id  334 non-null    object \n",
      " 17  photos                  722 non-null    object \n",
      " 18  primary_photo_cropped   705 non-null    object \n",
      " 19  videos                  722 non-null    object \n",
      " 20  status                  722 non-null    object \n",
      " 21  status_changed_at       722 non-null    object \n",
      " 22  published_at            722 non-null    object \n",
      " 23  distance                0 non-null      float64\n",
      " 24  contact                 722 non-null    object \n",
      " 25  _links                  722 non-null    object \n",
      "dtypes: float64(1), int64(1), object(24)\n",
      "memory usage: 146.8+ KB\n",
      "None\n",
      "         id organization_id  \\\n",
      "0  48555420           IL599   \n",
      "1  48549950           FL249   \n",
      "2  48550360          NC1092   \n",
      "3  48550730           KY519   \n",
      "4  48552648           WI128   \n",
      "\n",
      "                                                 url type species  \\\n",
      "0  https://www.petfinder.com/dog/ruff-48555420/il...  Dog     Dog   \n",
      "1  https://www.petfinder.com/dog/delilah-48549950...  Dog     Dog   \n",
      "2  https://www.petfinder.com/dog/red-48550360/nc/...  Dog     Dog   \n",
      "3  https://www.petfinder.com/dog/louie-48550730/o...  Dog     Dog   \n",
      "4  https://www.petfinder.com/dog/kenny-48552648/w...  Dog     Dog   \n",
      "\n",
      "                                              breeds  \\\n",
      "0  {'primary': 'Terrier', 'secondary': 'Pug', 'mi...   \n",
      "1  {'primary': 'German Shepherd Dog', 'secondary'...   \n",
      "2  {'primary': 'Dogue de Bordeaux', 'secondary': ...   \n",
      "3  {'primary': 'Labrador Retriever', 'secondary':...   \n",
      "4  {'primary': 'Border Collie', 'secondary': 'Non...   \n",
      "\n",
      "                                              colors    age  gender  \\\n",
      "0  {'primary': 'Brindle', 'secondary': 'Brown / C...  Young    Male   \n",
      "1  {'primary': 'Black', 'secondary': 'Yellow / Ta...  Adult  Female   \n",
      "2  {'primary': 'Red / Chestnut / Orange', 'second...  Adult    Male   \n",
      "3  {'primary': 'None', 'secondary': 'None', 'tert...  Young    Male   \n",
      "4  {'primary': 'None', 'secondary': 'None', 'tert...  Adult    Male   \n",
      "\n",
      "          size  ... organization_animal_id  \\\n",
      "0        Small  ...                    NaN   \n",
      "1       Medium  ...                    NaN   \n",
      "2  Extra Large  ...              FRR-A-411   \n",
      "3       Medium  ...                    NaN   \n",
      "4       Medium  ...                    NaN   \n",
      "\n",
      "                                              photos  \\\n",
      "0  [{'small': 'https://dl5zpyw5k3jeb.cloudfront.n...   \n",
      "1  [{'small': 'https://dl5zpyw5k3jeb.cloudfront.n...   \n",
      "2  [{'small': 'https://dl5zpyw5k3jeb.cloudfront.n...   \n",
      "3  [{'small': 'https://dl5zpyw5k3jeb.cloudfront.n...   \n",
      "4  [{'small': 'https://dl5zpyw5k3jeb.cloudfront.n...   \n",
      "\n",
      "                               primary_photo_cropped videos     status  \\\n",
      "0  {'small': 'https://dl5zpyw5k3jeb.cloudfront.ne...     []    adopted   \n",
      "1  {'small': 'https://dl5zpyw5k3jeb.cloudfront.ne...     []    adopted   \n",
      "2  {'small': 'https://dl5zpyw5k3jeb.cloudfront.ne...     []  adoptable   \n",
      "3  {'small': 'https://dl5zpyw5k3jeb.cloudfront.ne...     []    adopted   \n",
      "4  {'small': 'https://dl5zpyw5k3jeb.cloudfront.ne...     []    adopted   \n",
      "\n",
      "           status_changed_at               published_at distance  \\\n",
      "0  2020-08-16 18:21:00+00:00  2020-07-21 20:52:42+00:00      NaN   \n",
      "1  2020-07-31 01:39:20+00:00  2020-07-21 13:17:11+00:00      NaN   \n",
      "2  2020-07-21 14:00:23+00:00  2020-07-21 14:00:23+00:00      NaN   \n",
      "3  2020-07-26 15:50:42+00:00  2020-07-21 20:05:39+00:00      NaN   \n",
      "4  2020-07-23 16:00:20+00:00  2020-07-21 17:17:43+00:00      NaN   \n",
      "\n",
      "                                             contact  \\\n",
      "0  {'email': 'starfishanimalrescuer@gmail.com', '...   \n",
      "1  {'email': 'adoptions@theanimalleague.org', 'ph...   \n",
      "2  {'email': 'adopt@freedom-ride.org', 'phone': '...   \n",
      "3  {'email': 'lhkyrescue@gmail.com', 'phone': 'No...   \n",
      "4  {'email': 'paws@northwoodshumanesociety.org', ...   \n",
      "\n",
      "                                              _links  \n",
      "0  {'self': {'href': '/v2/animals/48555420'}, 'ty...  \n",
      "1  {'self': {'href': '/v2/animals/48549950'}, 'ty...  \n",
      "2  {'self': {'href': '/v2/animals/48550360'}, 'ty...  \n",
      "3  {'self': {'href': '/v2/animals/48550730'}, 'ty...  \n",
      "4  {'self': {'href': '/v2/animals/48552648'}, 'ty...  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "                 id  distance\n",
      "count  7.220000e+02       0.0\n",
      "mean   4.855259e+07       NaN\n",
      "std    1.933383e+03       NaN\n",
      "min    4.854955e+07       NaN\n",
      "25%    4.855080e+07       NaN\n",
      "50%    4.855292e+07       NaN\n",
      "75%    4.855442e+07       NaN\n",
      "max    4.855553e+07       NaN\n",
      "id                          0\n",
      "organization_id             0\n",
      "url                         0\n",
      "type                        0\n",
      "species                     0\n",
      "breeds                      0\n",
      "colors                      0\n",
      "age                         0\n",
      "gender                      0\n",
      "size                        0\n",
      "coat                      325\n",
      "attributes                  0\n",
      "environment                 0\n",
      "tags                        0\n",
      "name                        0\n",
      "description                64\n",
      "organization_animal_id    388\n",
      "photos                      0\n",
      "primary_photo_cropped      17\n",
      "videos                      0\n",
      "status                      0\n",
      "status_changed_at           0\n",
      "published_at                0\n",
      "distance                  722\n",
      "contact                     0\n",
      "_links                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('data/pet_adoption.csv')\n",
    "\n",
    "# Quick overview\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.isnull().sum())  # Check missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "59018829",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Example: parse 'breeds' column from JSON string to dictionary\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mbreeds_parsed\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbreeds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract primary breed\u001b[39;00m\n\u001b[32m      8\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mprimary_breed\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mbreeds_parsed\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: d.get(\u001b[33m'\u001b[39m\u001b[33mprimary\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/furever-home/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Example: parse 'breeds' column from JSON string to dictionary\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mbreeds_parsed\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mbreeds\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract primary breed\u001b[39;00m\n\u001b[32m      8\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mprimary_breed\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mbreeds_parsed\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: d.get(\u001b[33m'\u001b[39m\u001b[33mprimary\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/decoder.py:344\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    340\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    342\u001b[39m \n\u001b[32m    343\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     end = _w(s, end).end()\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/decoder.py:360\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m \n\u001b[32m    358\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Example: parse 'breeds' column from JSON string to dictionary\n",
    "df['breeds_parsed'] = df['breeds'].apply(lambda x: json.loads(x) if isinstance(x, str) else {})\n",
    "\n",
    "# Extract primary breed\n",
    "df['primary_breed'] = df['breeds_parsed'].apply(lambda d: d.get('primary', 'Unknown'))\n",
    "\n",
    "# Similarly, parse 'colors', 'attributes', 'environment' etc.\n",
    "# You can extract specific keys or encode boolean flags\n",
    "\n",
    "# Example: fill missing values in 'coat' with 'Unknown'\n",
    "df['coat'] = df['coat'].fillna('Unknown')\n",
    "\n",
    "# Encode categorical columns\n",
    "df['species_encoded'] = df['species'].astype('category').cat.codes\n",
    "df['gender_encoded'] = df['gender'].astype('category').cat.codes\n",
    "df['size_encoded'] = df['size'].astype('category').cat.codes\n",
    "df['primary_breed_encoded'] = df['primary_breed'].astype('category').cat.codes\n",
    "\n",
    "# Drop columns you don't need\n",
    "df_clean = df.drop(columns=['id', 'organization_id', 'url', 'breeds', 'breeds_parsed', 'colors', 'attributes', 'environment', 'tags', 'photos', 'primary_photo_cropped', 'videos', 'description', 'organization_animal_id', '_links', 'contact', 'distance'])\n",
    "\n",
    "# Now decide your target variable, for example 'adoption_speed' if exists or create one\n",
    "\n",
    "print(df_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7c7eebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "PetType               0\n",
      "Breed                 0\n",
      "AgeMonths             0\n",
      "Color                 0\n",
      "Size                  0\n",
      "WeightKg              0\n",
      "Vaccinated            0\n",
      "HealthCondition       0\n",
      "TimeInShelterDays     0\n",
      "AdoptionFee           0\n",
      "PreviousOwner         0\n",
      "AdoptionLikelihood    0\n",
      "dtype: int64\n",
      "Accuracy: 0.8209\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Load dataset (replace path as needed)\n",
    "df = pd.read_csv('data/pet_adoption_data.csv')\n",
    "\n",
    "# Drop PetID column as it's not a feature\n",
    "df = df.drop('PetID', axis=1)\n",
    "\n",
    "# Encode categorical columns using LabelEncoder and save encoders\n",
    "categorical_cols = ['PetType', 'Breed', 'Color', 'Size']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Fit encoder on the original string values\n",
    "    df[col] = df[col].astype(str)\n",
    "    le.fit(df[col])\n",
    "    label_encoders[col] = le  # Save the encoder\n",
    "    df[col] = le.transform(df[col])\n",
    "\n",
    "# Check for missing values (should be zero)\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('AdoptionLikelihood', axis=1).values\n",
    "y = df['AdoptionLikelihood'].values\n",
    "\n",
    "# Scale features to [0,1] range\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            most_common = Counter(k_nearest_labels).most_common(1)\n",
    "            y_pred.append(most_common[0][0])\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# Instantiate and train KNN\n",
    "knn = KNN(k=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save scaler and label encoders for later use (e.g., in API)\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# Save training data for KNN predict (optional, to reload)\n",
    "with open('X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open('y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a76d348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save training data\n",
    "with open('X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open('y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a8aeb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "categorical_cols = ['PetType', 'Breed', 'Color', 'Size']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# After encoding, save label_encoders dictionary\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e707b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetType classes: ['Bird' 'Cat' 'Dog' 'Rabbit']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "print(\"PetType classes:\", label_encoders['PetType'].classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b51f8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load saved encoders and scaler\n",
    "with open('label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "with open('X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open('y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6212f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            most_common = Counter(k_nearest_labels).most_common(1)\n",
    "            y_pred.append(most_common[0][0])\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# Re-instantiate and load model\n",
    "knn = KNN(k=5)\n",
    "knn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "86aaf699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 'Exotic Finch' not in label encoder for 'Breed', using fallback: Golden Retriever\n",
      "Prediction (low likelihood expected): 0\n"
     ]
    }
   ],
   "source": [
    "def safe_encode(col, value):\n",
    "    le = label_encoders[col]\n",
    "    if value in le.classes_:\n",
    "        return le.transform([value])[0]\n",
    "    else:\n",
    "        print(f\"[WARN] '{value}' not in label encoder for '{col}', using fallback: {le.classes_[0]}\")\n",
    "        return le.transform([le.classes_[0]])[0]\n",
    "\n",
    "pet_data_0 = {\n",
    "    \"PetType\": \"Dog\",              # must exist in label encoder\n",
    "    \"Breed\": \"Exotic Finch\",        # will fallback\n",
    "    \"Color\": \"Gray\",                # will fallback if needed\n",
    "    \"Size\": \"Small\",                # must exist\n",
    "    \"AgeMonths\": 72,\n",
    "    \"WeightKg\": 0.1,\n",
    "    \"Vaccinated\": 0,\n",
    "    \"HealthCondition\": 2,\n",
    "    \"TimeInShelterDays\": 150,\n",
    "    \"AdoptionFee\": 500,\n",
    "    \"PreviousOwner\": 1\n",
    "}\n",
    "\n",
    "# Encode safely\n",
    "for col in ['PetType', 'Breed', 'Color', 'Size']:\n",
    "    pet_data_0[col] = safe_encode(col, pet_data_0[col])\n",
    "\n",
    "# Predict\n",
    "features_0 = np.array([list(pet_data_0.values())])\n",
    "scaled_0 = scaler.transform(features_0)\n",
    "prediction_0 = knn.predict(scaled_0)[0]\n",
    "print(\"Prediction (low likelihood expected):\", prediction_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "51728a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (High likelihood expected): 1\n"
     ]
    }
   ],
   "source": [
    "# Input: try to pick values that are usually adopted quickly\n",
    "pet_data_1 = {\n",
    "    \"PetType\": \"Dog\",\n",
    "    \"Breed\": \"Labrador\",\n",
    "    \"Color\": \"Black\",\n",
    "    \"Size\": \"Medium\",\n",
    "    \"AgeMonths\": 4,\n",
    "    \"WeightKg\": 12.0,\n",
    "    \"Vaccinated\": 1,\n",
    "    \"HealthCondition\": 0,\n",
    "    \"TimeInShelterDays\": 3,\n",
    "    \"AdoptionFee\": 0,\n",
    "    \"PreviousOwner\": 0\n",
    "}\n",
    "\n",
    "# Encode categorical features\n",
    "for col in ['PetType', 'Breed', 'Color', 'Size']:\n",
    "    pet_data_1[col] = label_encoders[col].transform([pet_data_1[col]])[0]\n",
    "\n",
    "# Prepare for prediction\n",
    "features_1 = np.array([list(pet_data_1.values())])\n",
    "scaled_1 = scaler.transform(features_1)\n",
    "\n",
    "# Predict\n",
    "prediction_1 = knn.predict(scaled_1)[0]\n",
    "print(\"Prediction (High likelihood expected):\", prediction_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b43141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2007 entries, 0 to 2006\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   PetType             2007 non-null   object \n",
      " 1   Breed               2007 non-null   object \n",
      " 2   AgeMonths           2007 non-null   int64  \n",
      " 3   Color               2007 non-null   object \n",
      " 4   Size                2007 non-null   object \n",
      " 5   WeightKg            2007 non-null   float64\n",
      " 6   Vaccinated          2007 non-null   int64  \n",
      " 7   HealthCondition     2007 non-null   int64  \n",
      " 8   TimeInShelterDays   2007 non-null   int64  \n",
      " 9   AdoptionFee         2007 non-null   int64  \n",
      " 10  PreviousOwner       2007 non-null   int64  \n",
      " 11  AdoptionLikelihood  2007 non-null   int64  \n",
      "dtypes: float64(1), int64(7), object(4)\n",
      "memory usage: 188.3+ KB\n",
      "None\n",
      "  PetType             Breed  AgeMonths   Color    Size   WeightKg  Vaccinated  \\\n",
      "0    Bird          Parakeet        131  Orange   Large   5.039768           1   \n",
      "1  Rabbit            Rabbit         73   White   Large  16.086727           0   \n",
      "2     Dog  Golden Retriever        136  Orange  Medium   2.076286           0   \n",
      "3    Bird          Parakeet         97   White   Small   3.339423           0   \n",
      "4  Rabbit            Rabbit        123    Gray   Large  20.498100           0   \n",
      "\n",
      "   HealthCondition  TimeInShelterDays  AdoptionFee  PreviousOwner  \\\n",
      "0                0                 27          140              0   \n",
      "1                0                  8          235              0   \n",
      "2                0                 85          385              0   \n",
      "3                0                 61          217              1   \n",
      "4                0                 28           14              1   \n",
      "\n",
      "   AdoptionLikelihood  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n",
      "AdoptionLikelihood\n",
      "0    1348\n",
      "1     659\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Load dataset (replace path as needed)\n",
    "df = pd.read_csv('data/pet_adoption_data.csv')\n",
    "\n",
    "# Drop PetID column as it's not a feature\n",
    "df = df.drop('PetID', axis=1)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df['AdoptionLikelihood'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
